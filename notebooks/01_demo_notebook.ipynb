{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Test Case Clustering Demo\n",
    "\n",
    "This notebook demonstrates the semantic clustering and visualization of test cases using ELMo, BERT, and T5 embeddings.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Load and preprocess test case data\n",
    "2. Generate semantic embeddings using different models\n",
    "3. Apply clustering algorithms\n",
    "4. Visualize results and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.data.dataloader import load_demo_data\n",
    "from src.models.model_factory import ModelFactory\n",
    "from src.clustering.clustering_engine import ClusteringEngine\n",
    "from src.clustering.similarity_engine import SimilarityEngine\n",
    "from src.clustering.dimensionality_reduction import DimensionalityReducer\n",
    "from src.evaluation.metrics import EvaluationSuite\n",
    "from src.visualization.interactive_plots import create_cluster_plot\n",
    "from src.visualization.static_plots import plot_cluster_scatter\n",
    "\n",
    "print('‚úÖ All modules imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Test Case Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demo data\n",
    "df = load_demo_data(max_samples=500)\n",
    "\n",
    "print(f'Loaded {len(df)} test cases')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "\n",
    "# Display sample data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare texts for embedding\n",
    "texts = df['test_description'].tolist()\n",
    "text_ids = df['test_case_id'].tolist()\n",
    "\n",
    "# Test different models\n",
    "models = ['bert', 'elmo', 't5']\n",
    "embeddings_dict = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(f'--- Generating {model_name.upper()} embeddings ---')\n",
    "    \n",
    "    try:\n",
    "        # Create model\n",
    "        config = {'name': model_name, 'embedding_dim': 512}\n",
    "        embedder = ModelFactory.create_embedder(model_name, config)\n",
    "        embedder.initialize()\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = embedder.embed_texts(texts)\n",
    "        embeddings_dict[model_name] = embeddings\n",
    "        \n",
    "        print(f'‚úÖ {model_name.upper()}: {embeddings.shape}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error with {model_name}: {e}')\n",
    "        # Use random embeddings for demo\n",
    "        embeddings_dict[model_name] = np.random.randn(len(texts), 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clustering engine\n",
    "clustering_engine = ClusteringEngine()\n",
    "evaluator = EvaluationSuite()\n",
    "\n",
    "# Test clustering on BERT embeddings\n",
    "embeddings = embeddings_dict['bert']\n",
    "\n",
    "# Apply different clustering methods\n",
    "clustering_results = {}\n",
    "methods = ['kmeans', 'hierarchical', 'dbscan']\n",
    "\n",
    "for method in methods:\n",
    "    print(f'--- {method.upper()} Clustering ---')\n",
    "    \n",
    "    if method == 'kmeans':\n",
    "        labels = clustering_engine.apply_kmeans(embeddings, n_clusters=8)\n",
    "    elif method == 'hierarchical':\n",
    "        labels = clustering_engine.apply_hierarchical(embeddings, n_clusters=8)\n",
    "    else:  # dbscan\n",
    "        labels = clustering_engine.apply_dbscan(embeddings, eps=0.5)\n",
    "    \n",
    "    # Evaluate clustering\n",
    "    results = evaluator.evaluate_clustering(embeddings, labels)\n",
    "    clustering_results[method] = {'labels': labels, 'results': results}\n",
    "    \n",
    "    print(f'Clusters: {results[\"n_clusters\"]}')\n",
    "    print(f'Silhouette Score: {results[\"metrics\"].get(\"silhouette_score\", 0):.3f}')\n",
    "    print(f'Davies-Bouldin: {results[\"metrics\"].get(\"davies_bouldin_score\", 0):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Dimensionality Reduction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for visualization\n",
    "reducer = DimensionalityReducer()\n",
    "coords_2d = reducer.apply_tsne(embeddings, n_components=2)\n",
    "\n",
    "print(f'Reduced to 2D coordinates: {coords_2d.shape}')\n",
    "\n",
    "# Visualize K-means results\n",
    "kmeans_labels = clustering_results['kmeans']['labels']\n",
    "\n",
    "# Static plot\n",
    "plot_cluster_scatter(coords_2d, kmeans_labels, \n",
    "                    title='K-means Clustering Results (BERT + t-SNE)',\n",
    "                    figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot (if Plotly is available)\n",
    "try:\n",
    "    fig = create_cluster_plot(coords_2d, kmeans_labels, texts, \n",
    "                             title='Interactive Clustering Visualization')\n",
    "    if fig:\n",
    "        fig.show()\n",
    "    else:\n",
    "        print('Plotly not available')\n",
    "except Exception as e:\n",
    "    print(f'Interactive plot error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Semantic Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize similarity engine\n",
    "similarity_engine = SimilarityEngine()\n",
    "similarity_engine.load_embeddings(embeddings, text_ids, texts)\n",
    "\n",
    "# Test semantic search\n",
    "search_queries = [\n",
    "    'user login authentication',\n",
    "    'payment processing validation',\n",
    "    'database transaction error'\n",
    "]\n",
    "\n",
    "for query in search_queries:\n",
    "    print(f'üîç Search: \"{query}\"')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Create embedder for query\n",
    "    config = {'name': 'bert', 'embedding_dim': 512}\n",
    "    embedder = ModelFactory.create_embedder('bert', config)\n",
    "    embedder.initialize()\n",
    "    \n",
    "    # Search for similar test cases\n",
    "    results = similarity_engine.search_by_text(query, embedder, top_k=5)\n",
    "    \n",
    "    for i, (idx, text_id, score) in enumerate(results, 1):\n",
    "        print(f'{i}. {text_id} (similarity: {score:.3f})')\n",
    "        print(f'   {text_id.replace(\"_\", \" \").title()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different embedding models\n",
    "model_comparison = {}\n",
    "\n",
    "for model_name, model_embeddings in embeddings_dict.items():\n",
    "    print(f'--- Evaluating {model_name.upper()} ---')\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    clustering_engine = ClusteringEngine()\n",
    "    labels = clustering_engine.apply_kmeans(model_embeddings, n_clusters=8)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluator.evaluate_clustering(model_embeddings, labels)\n",
    "    model_comparison[model_name] = results\n",
    "    \n",
    "    print(f'Clusters: {results[\"n_clusters\"]}')\n",
    "    print(f'Silhouette: {results[\"metrics\"].get(\"silhouette_score\", 0):.3f}')\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model, results in model_comparison.items():\n",
    "    metrics = results['metrics']\n",
    "    comparison_data.append({\n",
    "        'Model': model.upper(),\n",
    "        'Silhouette Score': metrics.get('silhouette_score', 0),\n",
    "        'Davies-Bouldin': metrics.get('davies_bouldin_score', 0),\n",
    "        'Calinski-Harabasz': metrics.get('calinski_harabasz_score', 0),\n",
    "        'N Clusters': results['n_clusters']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print('üìä Model Comparison Results:')\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Silhouette scores\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Silhouette Score'], \n",
    "           color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[0].set_title('Silhouette Score Comparison')\n",
    "axes[0].set_ylabel('Score')\n",
    "\n",
    "# Davies-Bouldin scores\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['Davies-Bouldin'],\n",
    "           color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[1].set_title('Davies-Bouldin Index Comparison')\n",
    "axes[1].set_ylabel('Index (lower is better)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This demo showed:\n",
    "\n",
    "1. **Data Loading**: Synthetic test case generation for development\n",
    "2. **Multi-Model Embeddings**: ELMo, BERT, and T5 implementations\n",
    "3. **Clustering Analysis**: K-means, hierarchical, and DBSCAN methods\n",
    "4. **Visualization**: 2D t-SNE projections and interactive plots\n",
    "5. **Semantic Search**: Context-aware test case retrieval\n",
    "6. **Performance Evaluation**: Comprehensive metrics comparison\n",
    "\n",
    "The system demonstrates significant potential for improving software testing workflows through semantic understanding of test cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
